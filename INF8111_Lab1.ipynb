{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team\n",
    "\n",
    "Sofien Ben Ayed - 1971009\n",
    "\n",
    "Zhuofei Kang - 1939634\n",
    "\n",
    "Maryam El Arfaoui - 1939476\n",
    "\n",
    "\n",
    "# 1 - Overview\n",
    "\n",
    "Twitter is a mix of social network and microblogging. In this network, people post information and communicate among themselves through messages, called tweets, that can contain up to 280 characters. In this assignment, *we will implement a prototype that can detect if an airline company is positively or negatively mentioned in a tweet*. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "# 2 - Sentiment Analysis Model (13 points)\n",
    "\n",
    "In the literature, the task of extracting the sentiment of a text is called *sentiment analysis*. We will implement a bag-of-words (BoW) model for this task.\n",
    "\n",
    "## 2.1 -  Setup\n",
    "\n",
    "Please run the code below to install the packages needed for this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kkkan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kkkan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\kkkan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\kkkan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you want, you can use anaconda and install after nltk library\n",
    "# pip install --user numpy\n",
    "# pip install --user sklearn\n",
    "# pip install --user scipy\n",
    "# pip install --user nltk\n",
    "\n",
    "\n",
    "#python\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('universal_tagset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 - Dataset\n",
    "\n",
    "Please download the zip file in the following url: https://drive.google.com/file/d/1iGmESwPXpO3sIZFGOCrysxJ27AHdly-Y/view?usp=sharing\n",
    "\n",
    "In this zip file, there are 2 files:\n",
    "1. train.tsv: training dataset\n",
    "2. dev.tsv: validation dataset\n",
    "\n",
    "Each line of the files has the following information about a tweet: *tweet id*, *user id*, *label* and *message text*.\n",
    "\n",
    "There are three labels in the dataset: *negative*, *neutral* and *positive*. We represent each one of these labels as 0, 1 and 2 respectively.\n",
    "\n",
    "In the code above read the training and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import re\n",
    "\n",
    "def load_dataset(path):\n",
    "    dtFile = codecs.open(path, 'r')\n",
    "    \n",
    "    x = []\n",
    "    y = []\n",
    "    \n",
    "    for l in dtFile:\n",
    "        sid, uid, label,text = re.split(r\"\\s+\", l, maxsplit=3)\n",
    "        \n",
    "        text = text.strip()\n",
    "        \n",
    "        # Remove not available\n",
    "        if text == \"Not Available\":\n",
    "            continue\n",
    "        \n",
    "        x.append(text)\n",
    "        \n",
    "        if label == \"negative\": \n",
    "            y.append(0)\n",
    "        elif label == \"neutral\": \n",
    "            y.append(1)\n",
    "        elif label == \"positive\": \n",
    "            y.append(2)\n",
    "        \n",
    "    assert len(x) == len(y)\n",
    "            \n",
    "    return x,y\n",
    "            \n",
    "\n",
    "# Path of training dataset\n",
    "trainingPath=\"C:/Anaconda/data_mining/sentiment_analysis/train_data.tsv\"\n",
    "\n",
    "# Path of validation dataset\n",
    "validationPath=\"C:/Anaconda/data_mining/sentiment_analysis/dev_data.tsv\"\n",
    "\n",
    "training_X, training_Y = load_dataset(trainingPath)\n",
    "validation_X, validation_Y = load_dataset(validationPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 - Preprocessing\n",
    "\n",
    "Preprocessing is a crucial task in data mining. This task clean and transform the raw data in a format that can better suit data analysis and machine learning techniques. In natural language processing (NLP), *tokenization* and *stemming* are two well known preprocessing steps. Besides these two steps, we will implement an additional step that is designed exclusively for the twitter domain.\n",
    "\n",
    "### 2.3.1 - Tokenization\n",
    "\n",
    "In this preprocessing step, a *tokenizer* is responsible for breaking a text in a sequence of tokens (words, symbols, and punctuations). For instance, the sentence *\"It's the student's notebook.\"* can be split into the following list of tokens: ['It', \"'s\", 'the', 'student', \"'s\", 'notebook', '.'].\n",
    "\n",
    "\n",
    "#### 2.3.1.1 - Question 1 (0.5 point) \n",
    "\n",
    "Implement the SpaceTokenizer and NLTKTokenizer tokenizers: \n",
    "- **SpaceTokenizer** tokenizes the tokens that are separated by whitespace (space, tab, newline). This is a naive tokenizer.\n",
    "- **NLTKTokenizer** uses the default method of the nltk package (https://www.nltk.org/api/nltk.html) to tokenize the text.\n",
    "\n",
    "**All tokenizers have to lowercase the tokens.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "class SpaceTokenizer(object):\n",
    "    \"\"\"\n",
    "    It tokenizes the tokens that are separated by whitespace (space, tab, newline). \n",
    "    We consider that any tokenization was applied in the text when we use this tokenizer.\n",
    "    \n",
    "    For example: \"hello\\tworld of\\nNLP\" is split in ['hello', 'world', 'of', 'NLP']\n",
    "    \"\"\"\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "\n",
    "        tokens = text.replace('\\n',' ').replace('\\t',' ').split()\n",
    "        \n",
    "        # Have to return a list of tokens\n",
    "        return tokens\n",
    "        \n",
    "class NLTKTokenizer(object):\n",
    "    \"\"\"\n",
    "    This tokenizer uses the default function of nltk package (https://www.nltk.org/api/nltk.html) to tokenize the text.\n",
    "    \"\"\"\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        \n",
    "        # Have to return a list of tokens\n",
    "        return tokens\n",
    "\n",
    "# test = NLTKTokenizer()\n",
    "# print(test.tokenize(\"hello\\tworld of\\nNLP\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 - Stemming\n",
    "\n",
    "In the tweets *\"I should have bought a new shoes today\"* and *\"I spent too much money buying games\"*, the words *\"buy\"* and *\"bought\"* represent basically the same concept. Considering both words as different can unnecessarily increase the dimensionality of the problem and can negatively impact the performance of simple models. Therefore, a unique form (e.g., the root buy) can represent both words. The process to convert words with the same stem (word reduction that keeps word prefixes) to a standard form is called *stemming*.\n",
    "\n",
    "#### 2.3.2.1 - Question 2 (0.5 point) \n",
    "\n",
    "Retrieve the stems of the tokens using the attribute *stemmer* from the class *Stemmer*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "class Stemmer(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "    \n",
    "    def stem(self, tokens):\n",
    "        \"\"\"\n",
    "        tokens: a list of strings\n",
    "        \"\"\"\n",
    "        stemTokens = []\n",
    "        for word in tokens:\n",
    "            stemTokens.append(self.stemmer.stem(word))\n",
    "        # Have to return a list of stems\n",
    "        return stemTokens\n",
    "\n",
    "# test = Stemmer()\n",
    "# print(test.stem([\"hello\", \"fair\", \"fairly\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3 - Twitter preprocessing\n",
    "\n",
    "Sometimes only applying the default NLP preprocessing steps is not enough. Data for certain domains can have peculiar characteristics which requires specific preprocessing steps to remove the noise and create a more suitable format for the models. \n",
    "\n",
    "In NLP, methods store a set of words, called dictionary, and all the words out of the dictionary are considered as unknown. In this assignment, the feature space dimensionality of a model is directly related to the number of words in the dictionary. Since high-dimensional spaces can suffer from the curse of dimensionality, our goal is to create preprocessing steps that decrease vocabulary size.  \n",
    "\n",
    "#### 2.3.3.1 - Question 3 (2.0 points)\n",
    "\n",
    "Briefly explain and implement at least two preprocessing steps that reduce the dictionary size (number of unique words). These preprocessing steps must be related to the specific characteristic of the Twitter data. Therefore, for instance, the stop words removal will not be accepted as a preprocessing step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwitterPreprocessing(object):\n",
    "    \n",
    "    def preprocess(self, tweet):\n",
    "        \"\"\"\n",
    "        tweet: original tweet\n",
    "        \"\"\"\n",
    "        \n",
    "        # Remove every mentions to another profile\n",
    "        # Indeed, the name of a profile doesn't give much information and just increases the dictionary size\n",
    "        # On Twitter, a mention always begin by \"@\"\n",
    "        tweet = ' '.join(word for word in tweet.split() if word[0]!='@')\n",
    "        \n",
    "        # Remove every tags\n",
    "        # Indeed, most tags are created by the users and are an aggregation of several words\n",
    "        # Eliminate them reduce the dictionary size but a better way would be to decompose the words if there are\n",
    "        # On Twitter, a tag always begin by \"#\"\n",
    "        tweet = ' '.join(word for word in tweet.split() if word[0]!='#')\n",
    "        \n",
    "        # return the preprocessed twitter\n",
    "        return tweet\n",
    "\n",
    "# test = TwitterPreprocessing()\n",
    "# print(test.preprocess(\"Hello @jo it's me ? #funnyhere\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3  Pipeline\n",
    "\n",
    "The pipeline is sequence of preprocessing steps that transform the raw data to a format that is suitable for your problem. We implement the class *PreprocessingPipeline* that apply the tokenizer, twitter preprocessing and stemer to the text.\n",
    "\n",
    "**Feel free to change the preprocessing order.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessingPipeline:\n",
    "    \n",
    "    def __init__(self, tokenization, twitterPreprocessing, stemming):\n",
    "        \"\"\"\n",
    "        tokenization: enable or disable tokenization.\n",
    "        twitterPreprocessing: enable or disable twitter preprocessing.\n",
    "        stemming: enable or disable stemming.\n",
    "        \"\"\"\n",
    "\n",
    "        self.tokenizer= NLTKTokenizer() if tokenization else SpaceTokenizer()\n",
    "        self.twitterPreprocesser = TwitterPreprocessing() if twitterPreprocessing else None\n",
    "        self.stemmer = Stemmer() if stemming else None\n",
    "    \n",
    "    def preprocess(self, tweet):\n",
    "        \"\"\"\n",
    "        Transform the raw data\n",
    "\n",
    "        tokenization: boolean value.\n",
    "        twitterPreprocessing: boolean value. Apply the\n",
    "        stemming: boolean value.\n",
    "        \"\"\"\n",
    "        if self.twitterPreprocesser:\n",
    "            tweet = self.twitterPreprocesser.preprocess(tweet)\n",
    "        \n",
    "        tokens = self.tokenizer.tokenize(tweet)\n",
    "\n",
    "        if self.stemmer:\n",
    "            tokens = self.stemmer.stem(tokens)\n",
    "            \n",
    "        return tokens\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 N-grams\n",
    "\n",
    "An n-gram is a contiguous sequence of *n* tokens from a text. Thus, for instance,the sequence *\"bye as\"* and *\"walked through\"* are example of 2-grams from the sentence *\"He said bye as he walked through the door .\"*. 1-gram, 2-gram and 3-gram are, respectively, called unigram, bigram and trigram. We list all the possible unigram, bigram and trigram from the *\"He said bye as he walked through the door .\"*:\n",
    "\n",
    "- Unigram: [\"He\", \"said\", \"bye\", \"as\", \"he\", \"walked\", \"through\", \"the\", \"door\", \".\"]\n",
    "- Bigram: [\"He said\", \"said bye\", \"bye as\", \"as he\", \"he walked\", \"walked through\", \"through the\", \"the door\", \"door .\"] \n",
    "- Trigram: [\"He said bye\", \"said bye as\", \"bye as he\", \"as he walked\", \"he walked through\", \"walked through the\", \"through the door\", \"the door .\"] \n",
    "\n",
    "\n",
    "### 2.4.1 - Question 4 (1 point)\n",
    "\n",
    "Implement bigram and trigram.\n",
    "\n",
    "**For this exercise, you cannot use any external python library (e.g., scikit-learn).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram(tokens):\n",
    "    \"\"\"\n",
    "    tokens: a list of strings\n",
    "    \"\"\"\n",
    "    bigrams = []\n",
    "    \n",
    "    for i in range(len(tokens)-1):\n",
    "        bigrams.append(tokens[i] + \" \" + tokens[i+1])\n",
    "        \n",
    "    return bigrams\n",
    "\n",
    "def trigram(tokens):\n",
    "    \"\"\"\n",
    "    tokens: a list of strings\n",
    "    \"\"\"\n",
    "    trigrams = []\n",
    "    \n",
    "    for i in range(len(tokens)-2):\n",
    "        trigrams.append(tokens[i] + \" \" + tokens[i+1] + \" \" + tokens[i+2])\n",
    "        \n",
    "    return trigrams\n",
    "    \n",
    "# test = [\"He\", \"said\", \"bye\", \"as\", \"he\", \"walked\", \"through\", \"the\", \"door\", \".\"]\n",
    "# print(bigram(test))\n",
    "# print(trigram(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Bag-of-words\n",
    "\n",
    "Logistic regression, SVM and other well-known models only accept inputs that have the same size. However, there are some data types whose sizes are not fixed, for instance, a text can have an unlimited number of words. Imagine that we retrieve two tweets: ”Board games are much better than video games” and ”Pandemic is an awesome game!”. These sentences are respectively named as Sentence 1 and 2. Table below depicts how we could represent both sentences using a fixed representation.\n",
    "\n",
    "|            | an | are | ! | pandemic | awesome | better | games | than | video | much | board | is | game |\n",
    "|------------|----|-----|---|----------|---------|--------|-------|------|-------|------|-------|----|------|\n",
    "| Sentence 1 | 0  | 1   | 0 | 0        | 0       | 1      | 2     | 1    | 1     | 1    | 1     | 0  | 0    |\n",
    "| Sentence 2 | 1  | 0   | 0 | 1        | 1       | 0      | 0     | 0    | 0     | 0    | 0     | 1  | 1    |\n",
    "\n",
    "Each column of this table 2.1 represents one of 13 vocabulary words, whereas the rows contains the word\n",
    "frequencies in each sentence. For instance, the cell in row 1 and column 7 has the value 2\n",
    "because the word games occurs twice in Sentence 1. Since the rows have always 13 values, we\n",
    "could use those vectors to represent the Sentences 1 and 2. The table above illustrates a technique called bag-of-words. Bag-of-words represents a document as a vector whose dimensions are equal to the number of times that vocabulary words appeared in the document. Thus, each token will be related to a dimension, i.e., an integer.\n",
    "\n",
    "### 2.5.1 - Question 5 (2 points)\n",
    "\n",
    "Implement the bag-of-words model that weights the vector with the absolute word frequency.\n",
    "\n",
    "**For this exercise, you cannot use any external python library (e.g., scikit-learn). However, if you have a problem with memory size, you can use the class scipy.sparse.csr_matrix (https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csc_matrix.html)\n",
    "**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "class CountBoW(object):\n",
    "    \n",
    "    def __init__(self, pipeline, bigram=False, trigram=False, words_indexed={}):\n",
    "        \"\"\"\n",
    "        pipelineObj: instance of PreprocesingPipeline\n",
    "        bigram: enable or disable bigram\n",
    "        trigram: enable or disable trigram\n",
    "        \"\"\"\n",
    "        self.pipeline = pipeline\n",
    "        self.bigram = bigram\n",
    "        self.trigram = trigram\n",
    "        self.words_indexed = words_indexed\n",
    "        \n",
    "    def fit_transform(self, X):\n",
    "        \"\"\"\n",
    "        This method preprocesses the data using the pipeline object, relates each unigram, bigram or trigram \n",
    "        to a specific integer and transforms the text in a vector. \n",
    "        Vectors are weighted using the token frequencies in the sentence.\n",
    "        \n",
    "        X: a list that contains tweet contents\n",
    "        \n",
    "        :return: a list of vectors\n",
    "        \"\"\"\n",
    "        \n",
    "        # A list of lists of tokens of each tweet\n",
    "        tweets_tokenized = []\n",
    "        \n",
    "        for tweet in X:\n",
    "            # Preprocess of data\n",
    "            tokens = self.pipeline.preprocess(tweet)\n",
    "            \n",
    "            # List of tokens of the current tweet\n",
    "            all_tokens = []\n",
    "            \n",
    "            for token in tokens:\n",
    "                all_tokens.append(token)\n",
    "            \n",
    "            if self.bigram:\n",
    "                # Adding bigrams\n",
    "                bigrams = bigram(tokens)\n",
    "                for token in bigrams:\n",
    "                    all_tokens.append(token)\n",
    "                    \n",
    "            if self.trigram:\n",
    "                # Adding trigrams\n",
    "                trigrams = trigram(tokens)\n",
    "                for token in trigrams:\n",
    "                    all_tokens.append(token)\n",
    "                    \n",
    "            tweets_tokenized.append(all_tokens)\n",
    "        \n",
    "        # The vocabulary is put in a set\n",
    "        set_all_words = set()\n",
    "        for tweet in tweets_tokenized:\n",
    "            for word in tweet:\n",
    "                set_all_words.add(word)\n",
    "        \n",
    "        # Each word of the vocabulary is associated to an integer\n",
    "        all_words = list(set_all_words)\n",
    "        words_indexed = {}\n",
    "        i = 0\n",
    "        for word in all_words:\n",
    "            words_indexed[word] = i\n",
    "            i += 1\n",
    "        \n",
    "        # The vocabulary-integer associations are saved\n",
    "        self.words_indexed = words_indexed\n",
    "        \n",
    "        # Every tweet is converted to bag-of-words vector\n",
    "        indptr = [0]\n",
    "        indices = []\n",
    "        data = []\n",
    "\n",
    "        for tweet in tweets_tokenized:\n",
    "            for token in tweet:\n",
    "                index = words_indexed[token]\n",
    "                indices.append(index)\n",
    "                data.append(1)\n",
    "            indptr.append(len(indices))\n",
    "        \n",
    "        bow = csr_matrix((data, indices, indptr), dtype=int).toarray()\n",
    "        \n",
    "        return bow\n",
    "        \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        This method preprocesses the data using the pipeline object and  transforms the text in a list of integer.\n",
    "        Vectors are weighted using the token frequencies in the sentence.\n",
    "        \n",
    "        X: a list of vectors\n",
    "        \n",
    "        :return: a list of vectors\n",
    "        \"\"\"\n",
    "        \n",
    "        # A list of lists of tokens of each tweet\n",
    "        tweets_tokenized = []\n",
    "        \n",
    "        for tweet in X:\n",
    "            # Preprocess of data\n",
    "            tokens = self.pipeline.preprocess(tweet)\n",
    "            \n",
    "            # List of tokens of the current tweet\n",
    "            all_tokens = []\n",
    "            \n",
    "            for token in tokens:\n",
    "                all_tokens.append(token)\n",
    "            \n",
    "            if self.bigram:\n",
    "                # Adding bigrams\n",
    "                bigrams = bigram(tokens)\n",
    "                for token in bigrams:\n",
    "                    all_tokens.append(token)\n",
    "                    \n",
    "            if self.trigram:\n",
    "                # Adding trigrams\n",
    "                trigrams = trigram(tokens)\n",
    "                for token in trigrams:\n",
    "                    all_tokens.append(token)\n",
    "                    \n",
    "            tweets_tokenized.append(all_tokens)\n",
    "        \n",
    "        indptr = [0]\n",
    "        indices = []\n",
    "        data = []\n",
    "\n",
    "        for tweet in tweets_tokenized:\n",
    "            for token in tweet:\n",
    "                if token in self.words_indexed:\n",
    "                    index = self.words_indexed[token]\n",
    "                    indices.append(index)\n",
    "                    data.append(1)\n",
    "            indptr.append(len(indices))\n",
    "        \n",
    "        bow = csr_matrix((data, indices, indptr), dtype=int, shape=(len(tweets_tokenized), len(self.words_indexed))).toarray()\n",
    "        \n",
    "        return bow\n",
    "      \n",
    "        \n",
    "# testSentences = [\"Board games are much better than video games\", \"Pandemic is an awesome game!\"]\n",
    "# test = CountBoW(PreprocessingPipeline(True, True, True))\n",
    "# validation = [\"I love this board game!\"]\n",
    "# test1 = test.fit_transform(testSentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.2 - TF-IDF\n",
    "\n",
    "Using raw frequency in the bag-of-words can be problematic. The word frequency distribution\n",
    "is skewed - only a few words have high frequencies in a document. Consequently, the\n",
    "weight of these words will be much bigger than the other ones which can give them more\n",
    "impact on some tasks, like similarity comparison. Besides that, a set of words (including\n",
    "those with high frequency) appears in most of the documents and, therefore, they do not\n",
    "help to discriminate documents. For instance, the word *of* appears in a significant\n",
    "part of tweets. Thus, having the word *of* does not make\n",
    "documents more or less similar. However, the word *terrible* is rarer and documents that\n",
    "have this word are more likely to be negative. TF-IDF is a technique that overcomes the word frequency disadvantages.\n",
    "\n",
    "TF-IDF weights the vector using inverse document frequency (IDF) and word frequency, called term frequency (TF).\n",
    "TF is the local information about how important is a word to a specific document.  IDF measures the discrimination level of the words in a dataset.  Common words in a domain are not helpful to discriminate documents since most of them contain these terms. So, to reduce their relevance in the documents, these words should have low weights in the vectors . \n",
    "The following equation calculates the word IDF:\n",
    "\\begin{equation}\n",
    "\tidf_i = \\log\\left( \\frac{N}{df_i} \\right),\n",
    "\\end{equation}\n",
    "where $N$ is the number of documents in the dataset, $df_i$ is the number of documents that contain a word $i$.\n",
    "The new weight $w_{ij}$ of a word $i$ in a document $j$ using TF-IDF is computed as:\n",
    "\\begin{equation}\n",
    "\tw_{ij} = tf_{ij} \\times idf_i,\n",
    "\\end{equation}\n",
    "where $tf_{ij}$ is the term frequency of word $i$ in the document $j$.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 2.5.2.1 - Question 6 (3 points)\n",
    "\n",
    "Implement a bag-of-words model that weights the vector using TF-IDF.\n",
    "\n",
    "**For this exercise, you cannot use any external python library (e.g., scikit-learn). However, if you have a problem with memory size, you can use the class scipy.sparse.csr_matrix (https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csc_matrix.html)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class TFIDFBoW(object):\n",
    "    \n",
    "    def __init__(self, pipeline, bigram=False, trigram=False, words_indexed={}, IDF=[]):\n",
    "        \"\"\"\n",
    "        pipelineObj: instance of PreprocesingPipeline\n",
    "        bigram: enable or disable bigram\n",
    "        trigram: enable or disable trigram\n",
    "        \"\"\"\n",
    "        self.pipeline = pipeline\n",
    "        self.bigram = bigram\n",
    "        self.trigram = trigram\n",
    "        self.words_indexed = words_indexed\n",
    "        self.IDF = IDF\n",
    "\n",
    "        \n",
    "    def fit_transform(self, X):\n",
    "        \"\"\"\n",
    "        This method preprocesses the data using the pipeline object, calculates the IDF and TF and \n",
    "        transforms the text in vectors. Vectors are weighted using TF-IDF method.\n",
    "        \n",
    "        X: a list that contains tweet contents\n",
    "        \n",
    "        :return: a list that contains the list of integers\n",
    "        \"\"\"\n",
    "        \n",
    "        # A list of lists of tokens of each tweet\n",
    "        tweets_tokenized = []\n",
    "        \n",
    "        for tweet in X:\n",
    "            # Preprocess of data\n",
    "            tokens = self.pipeline.preprocess(tweet)\n",
    "            \n",
    "            # List of tokens of the current tweet\n",
    "            all_tokens = []\n",
    "            \n",
    "            for token in tokens:\n",
    "                all_tokens.append(token)\n",
    "            \n",
    "            if self.bigram:\n",
    "                # Adding bigrams\n",
    "                bigrams = bigram(tokens)\n",
    "                for token in bigrams:\n",
    "                    all_tokens.append(token)\n",
    "                    \n",
    "            if self.trigram:\n",
    "                # Adding trigrams\n",
    "                trigrams = trigram(tokens)\n",
    "                for token in trigrams:\n",
    "                    all_tokens.append(token)\n",
    "                    \n",
    "            tweets_tokenized.append(all_tokens)\n",
    "        \n",
    "        # The vocabulary in a set\n",
    "        set_all_words = set()\n",
    "        for tweet in tweets_tokenized:\n",
    "            for word in tweet:\n",
    "                set_all_words.add(word)\n",
    "        \n",
    "        # Each word of the vocabulary is associated to an integer\n",
    "        all_words = list(set_all_words)\n",
    "        words_indexed = {}\n",
    "        i = 0\n",
    "        for word in all_words:\n",
    "            words_indexed[word] = i\n",
    "            i += 1\n",
    "        \n",
    "        # The vocabulary-integer associations are saved\n",
    "        self.words_indexed = words_indexed\n",
    "        \n",
    "        # Every tweet is converted to bag-of-words vector\n",
    "        indptr = [0]\n",
    "        indices = []\n",
    "        data = []\n",
    "\n",
    "        for tweet in tweets_tokenized:\n",
    "            for token in tweet:\n",
    "                index = words_indexed[token]\n",
    "                indices.append(index)\n",
    "                data.append(1)\n",
    "            indptr.append(len(indices))\n",
    "        \n",
    "        bow = csr_matrix((data, indices, indptr), dtype=int).toarray()\n",
    "        \n",
    "        # The number of tweets\n",
    "        N = len(tweets_tokenized)\n",
    "        \n",
    "        # For each word, the number of tweets cotaining it\n",
    "        IDF = []\n",
    "        for i in range(len(words_indexed)):\n",
    "            count = 0\n",
    "            for j in range(N):\n",
    "            #for bag in bow:\n",
    "                #if bag.toarray()[0][i] > 0:\n",
    "                if bow[j][i] > 0:\n",
    "                    count += 1\n",
    "            IDF.append(math.log(N/count))\n",
    "        \n",
    "        # The IDF for each word is saved\n",
    "        self.IDF = IDF\n",
    "        \n",
    "        indptr = [0]\n",
    "        indices = []\n",
    "        data = []\n",
    "\n",
    "        for bag in bow:\n",
    "            for index, count in enumerate(bag):\n",
    "                if count != 0 and IDF[index] != 0:\n",
    "                    indices.append(index)\n",
    "                    data.append(bag[index] * IDF[index])\n",
    "            indptr.append(len(indices))\n",
    "\n",
    "        bowTFIDF = csr_matrix((data, indices, indptr), dtype=float)#.toarray()\n",
    "        print(\"##4\")\n",
    "        return bowTFIDF\n",
    "        \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        This method preprocesses the data using the pipeline object and  \n",
    "            transforms the text in a list of integer.\n",
    "        \n",
    "        X: a list of vectors\n",
    "        \n",
    "        :return: a list of vectors\n",
    "        \"\"\"        \n",
    "        \n",
    "        # A list of lists of tokens of each tweet\n",
    "        tweets_tokenized = []\n",
    "        \n",
    "        for tweet in X:\n",
    "            # Preprocess of data\n",
    "            tokens = self.pipeline.preprocess(tweet)\n",
    "            \n",
    "            # List of tokens of the current tweet\n",
    "            all_tokens = []\n",
    "            \n",
    "            for token in tokens:\n",
    "                all_tokens.append(token)\n",
    "            \n",
    "            if self.bigram:\n",
    "                # Adding bigrams\n",
    "                bigrams = bigram(tokens)\n",
    "                for token in bigrams:\n",
    "                    all_tokens.append(token)\n",
    "                    \n",
    "            if self.trigram:\n",
    "                # Adding trigrams\n",
    "                trigrams = trigram(tokens)\n",
    "                for token in trigrams:\n",
    "                    all_tokens.append(token)\n",
    "                    \n",
    "            tweets_tokenized.append(all_tokens)\n",
    "\n",
    "        indptr = [0]\n",
    "        indices = []\n",
    "        data = []\n",
    "\n",
    "        for tweet in tweets_tokenized:\n",
    "            for token in tweet:\n",
    "                if token in self.words_indexed:\n",
    "                    index = self.words_indexed[token]\n",
    "                    indices.append(index)\n",
    "                    data.append(1)\n",
    "            indptr.append(len(indices))\n",
    "\n",
    "        bow = csr_matrix((data, indices, indptr), dtype=int, shape=(len(tweets_tokenized), len(self.words_indexed))).toarray()\n",
    "\n",
    "        indptr = [0]\n",
    "        indices = []\n",
    "        data = []\n",
    "        \n",
    "        for bag in bow:\n",
    "            for index, count in enumerate(bag):\n",
    "                if count != 0 and self.IDF[index] != 0:\n",
    "                    indices.append(index)\n",
    "                    data.append(bag[index] * self.IDF[index])\n",
    "            indptr.append(len(indices))\n",
    "        \n",
    "        bowTFIDF = csr_matrix((data, indices, indptr), dtype=float, shape=(len(tweets_tokenized), len(self.words_indexed)))#.toarray()\n",
    "        \n",
    "        return bowTFIDF\n",
    "\n",
    "# testSentences = [\"Board games are much better than video games\", \"Pandemic is an awesome game!\"]\n",
    "# test = TFIDFBoW(PreprocessingPipeline(True, True, True))\n",
    "# validation = [\"I love this board game!\"]\n",
    "# print(test.fit_transform(testSentences))\n",
    "# print(test.transform(validation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 - Classifier using BoW\n",
    "\n",
    "We are going to use logistic regression as a classifier. Read the following page to now more about this classifier: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "\n",
    "\n",
    "The method *train_evaluate* trains and evaluates the logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def train_evaluate(training_X, training_Y, validation_X, validation_Y, bowObj):\n",
    "    \"\"\"\n",
    "    training_X: tweets from the training dataset\n",
    "    training_Y: tweet labels from the training dataset\n",
    "    validation_X: tweets from the validation dataset\n",
    "    validation_Y: tweet labels from the validation dataset\n",
    "    bowObj: Bag-of-word object\n",
    "    \n",
    "    :return: the classifier and its accuracy in the training and validation dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    classifier = LogisticRegression()\n",
    "    \n",
    "    training_rep = bowObj.fit_transform(training_X)\n",
    "    #dicSize = len(training_rep[0])\n",
    "    dicSize = training_rep.get_shape()[1]\n",
    "    \n",
    "    classifier.fit(training_rep, training_Y)\n",
    "   \n",
    "    trainAcc = accuracy_score(training_Y,classifier.predict(training_rep))\n",
    "    validationAcc = accuracy_score(validation_Y,classifier.predict(bowObj.transform(validation_X)))\n",
    "    \n",
    "    return classifier, trainAcc, validationAcc, dicSize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2.6.1 - Question 7 (4 points)\n",
    "\n",
    "Train and calculate the logistic regression accuracy in the *training and validation dataset* using each one of the following configurations:\n",
    "    1. CountBoW + SpaceTokenizer(without tokenizer) + unigram \n",
    "    2. CountBoW + NLTKTokenizer + unigram\n",
    "    3. TFIDFBoW + NLTKTokenizer + unigram\n",
    "    3. TFIDFBoW + NLTKTokenizer + Stemming + unigram\n",
    "    4. TFIDFBoW + NLTKTokenizer + Twitter preprocessing + Stemming  + unigram\n",
    "    5. TFIDFBoW + NLTKTokenizer + Twitter preprocessing + Stemming  + unigram + bigram\n",
    "    6. TFIDFBoW + NLTKTokenizer + Twitter preprocessing + Stemming  + unigram + bigram + trigram\n",
    "Besides the accuracy, you have to report the dictionary size for each one of configurations. Finally, describe the results found by you and answer the following questions:\n",
    "- Which preprocessing has helped the model? Why?\n",
    "- TF-IDF has achieved a better performance than CountBoW? If yes, why do you think that this has occurred? \n",
    "- Has the bigram and trigram improved the performance? If yes, can you mention the reasons of this improvement?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programmes\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\Programmes\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:459: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "test1 = train_evaluate(training_X, training_Y, validation_X, validation_Y, CountBoW(PreprocessingPipeline(False, False, False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programmes\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\Programmes\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:459: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "test2 = train_evaluate(training_X, training_Y, validation_X, validation_Y, CountBoW(PreprocessingPipeline(True, False, False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##1\n",
      "##2\n",
      "##3\n",
      "##4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programmes\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\Programmes\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:459: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "test3 = train_evaluate(training_X, training_Y, validation_X, validation_Y, TFIDFBoW(PreprocessingPipeline(True, False, False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##1\n",
      "##2\n",
      "##3\n",
      "##4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programmes\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\Programmes\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:459: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "test4 = train_evaluate(training_X, training_Y, validation_X, validation_Y, TFIDFBoW(PreprocessingPipeline(True, False, True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##1\n",
      "##2\n",
      "##3\n",
      "##4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programmes\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\Programmes\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:459: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "test5 = train_evaluate(training_X, training_Y, validation_X, validation_Y, TFIDFBoW(PreprocessingPipeline(True, True, True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##1\n",
      "##2\n",
      "##3\n",
      "##4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programmes\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\Programmes\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:459: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "test6 = train_evaluate(training_X, training_Y, validation_X, validation_Y, TFIDFBoW(PreprocessingPipeline(True, True, True), True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##1\n",
      "##2\n",
      "##3\n",
      "##4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programmes\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\Programmes\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:459: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "test7 = train_evaluate(training_X, training_Y, validation_X, validation_Y, TFIDFBoW(PreprocessingPipeline(True, True, True), True, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9950124688279302, 0.9900249376558603, 0.9992665395335192, 0.9992665395335192, 0.9988264632536307, 0.9992665395335192, 0.9992665395335192]\n",
      "[0.5948808473080318, 0.6275375110326566, 0.6257722859664607, 0.6363636363636364, 0.6125330979699912, 0.6328331862312445, 0.6204766107678729]\n",
      "[32157, 24141, 24141, 17677, 14197, 90673, 208662]\n"
     ]
    }
   ],
   "source": [
    "trainAccs = [test1[1], test2[1], test3[1], test4[1], test5[1], test6[1], test7[1]]\n",
    "validationAccs = [test1[2], test2[2], test3[2], test4[2], test5[2], test6[2], test7[2]]\n",
    "dicSizes = [test1[3], test2[3], test3[3], test4[3], test5[3], test6[3], test7[3]]\n",
    "print(trainAccs)\n",
    "print(validationAccs)\n",
    "print(dicSizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training Accuracies** : \n",
    "\n",
    "[0.9950124688279302, 0.9900249376558603, 0.9992665395335192, 0.9992665395335192, 0.9988264632536307, 0.9992665395335192, 0.9992665395335192]\n",
    "\n",
    "**Validation Accuracies** :\n",
    "\n",
    "[0.5948808473080318, 0.6275375110326566, 0.6257722859664607, 0.6363636363636364, 0.6125330979699912, 0.6328331862312445, 0.6204766107678729]\n",
    "\n",
    "**Dictionary Sizes** : \n",
    "\n",
    "[32157, 24141, 24141, 17677, 14197, 90673, 208662]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results description** : \n",
    "\n",
    "-the first remark that can be made about the results is that the accuracies of the training file are high compared to those of the validation file, which is quite normal since the model uses the vocabulary of the training file whereas in the test file several significant terms can be overlooked. Also, we can say that there is overfitting because of the very high values of the training accuracies (close to 1).\n",
    "\n",
    "-we can also say that there is not a big difference between the accuracies of the different methods, however we notice higher values for methods 4 and 6.\n",
    "\n",
    "-Regarding the size of the dictionary, in fact, its size increases when we introduce the bigram and trigram, and we also note that stemming and twitterpreprossessing make a significant reduction in the size of the dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The best preprocessing methods are n°4 and n°6** :\n",
    "\n",
    "-> TFIDFBoW + NLTKTokenizer + Stemming + unigram\n",
    "\n",
    "-> TFIDFBoW + NLTKTokenizer + Twitter preprocessing + Stemming  + unigram + bigram\n",
    "\n",
    "This can be explained by the use of TF-IDF, bigrams and stemming.\n",
    "\n",
    "**Stemming** allows to reduce the size of the vocabulary and to associate several different words which have the same meaning.\n",
    "\n",
    "**TF-IDF can achieve a better performance than CountBoW**. Indeed, CountBoW only gives a vector with the number of occurences of each word. This way of counting can lead to some issues. For example, there are some words which are common to every piece of text as \"the\", \"a\", \"is\"... The number of occurences of these one will be high while they don't give us any clue regarding the sentiment expressed. \"Awful\" is much more interesting but less frequent and therefore has a lower weight in CountBoW. TF-IDF performs better by facing those issues. As the words like \"the\" and \"a\" appear in most of the tweets, thet will have low weights while \"awful\" is rarer and will be considered as more important.\n",
    "\n",
    "**Using n-grams can improve performance**. Unigrams are words alone but many times a unique word doesn't convey all the information and we use 2 or 3 words to get a complete sense. In our case of sentiment analysis, \"not like\" is negative but using unigrams will lead us to take into account only \"not\" and \"like\" separately. The same happens for locutions and expressions of 2/3 words or more. That's why by using bigrams and trigrams, it performs better : it can get a more global sense of the sentiment expressed.\n",
    "\n",
    "Finally, we can also say that using NLTKTokenizer is a better choice than our simple SpaceTokenizer : it takes punctuation into account which has as a consequence to reduce the dictionary size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kkkan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kkkan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestBowObj = TFIDFBoW(PreprocessingPipeline(True, False, True))\n",
    "bestClassifier = LogisticRegression()\n",
    "training_rep = bestBowObj.fit_transform(training_X)\n",
    "bestClassifier.fit(training_rep, training_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Prototype (7 points)\n",
    "\n",
    "During the last years, *E Corp* has collected tweets to create a dataset to their sentiment analysis tool. Now, airline companies have contracted *E Corp* to analyze the consumer opinion about them. Your job is to extract information from the tweet database about the following companies: Air France, American, British Airways,  Delta, Southwest, United, Us Airways and Virgin America.\n",
    "\n",
    "*For the prototype, you have to use the best model found in the Section 2.*\n",
    "\n",
    "## 3.1 Dataset\n",
    "\n",
    "In https://drive.google.com/file/d/1Cuw6Y12Bj91vF_iH49mqPZZfJkY92iBY/view?usp=sharing, you can find the raw tweet retrieved by E corp.  Each tweet is represented as json that the have attributes listed in the page https://developer.twitter.com/en/docs/tweets/data-dictionary/overview/tweet-object.\n",
    "\n",
    "** You will answer the question of this section using this tweet database (https://drive.google.com/file/d/1Cuw6Y12Bj91vF_iH49mqPZZfJkY92iBY/view?usp=sharing).**\n",
    "\n",
    "## 3.2 Sentiment Analysis\n",
    "\n",
    "\n",
    "### 3.2.1 Question 8 (0.5 point)\n",
    "\n",
    "Implement the method *extract_tweet_content* that extracts the content of each tweet in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def extract_tweet_content(raw_tweet_file):\n",
    "    \"\"\"\n",
    "    Extract the tweet content for each json object\n",
    "    \n",
    "    raw_tweet_file: file path that contains all json objects\n",
    "    \n",
    "    :return: a list with the tweet contents\n",
    "    \"\"\"\n",
    "    tweets = []\n",
    "    \n",
    "    with open(raw_tweet_file) as f:\n",
    "        for line in f:\n",
    "            tweets.append(json.loads(line)[\"text\"])\n",
    "    \n",
    "    return tweets\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Question 9 (1 points)\n",
    "\n",
    "Implement the method *detect_airline* that detects the airline companies in a tweet. Besides that, explain your approach to detect the companies and its possible drawbacks.\n",
    "\n",
    "The detect_airline has to be able to return if none or more than one airline companies are mentioned in a tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some explanations**\n",
    "\n",
    "A simple method is used to detect an airline company in a tweet.\n",
    "All the names are stored in a list and for each name in the list, we check the tweet which is a string, contains the name as a substring (case insensitive).\n",
    "One drawback of this method is the probability of false positive especially with words in the names of companies that can be used in other contexts.\n",
    "False negative can also be a problem if acronyms are often used or if people talk about a company without the last word \"airlines\". For example, \"I fly with united\" will give a false negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_airline(tweet):\n",
    "    \"\"\"\n",
    "    Detect and return the airline companies mentioned in the tweet\n",
    "    \n",
    "    tweet: represents the tweet message. Type : string\n",
    "    \n",
    "    :return: list of detected airline companies\n",
    "    \"\"\"\n",
    "    airline_companies = [\"air france\", \"american airlines\", \"british airways\", \"delta airlines\", \"southwest airlines\", \"united airlines\", \"us airways\", \"virgin america\"]\n",
    "    \n",
    "    detected = []\n",
    "    \n",
    "    # Not sensitive to uppercase/lowercase\n",
    "    tweet = tweet.lower()\n",
    "    \n",
    "    for company in airline_companies:\n",
    "        if company in tweet:\n",
    "            detected.append(company)\n",
    "    \n",
    "    # If any company is detected, it will return an empty list\n",
    "    return detected\n",
    "\n",
    "# test = \"Air france is better than delta\"\n",
    "# print(detect_airline(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 3.2.1 Question 10 (0.5 points)\n",
    "\n",
    "Implement the method *extract_sentiment* that receives a tweet and extracts its sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentiment(classifier, tweet, bowObj):\n",
    "    \"\"\"\n",
    "    Extract the tweet sentiment\n",
    "    \n",
    "    classifier: classifier object\n",
    "    tweet: represents the tweet message. Type : string\n",
    "    \n",
    "    :return: list of detected airline companies\n",
    "    \"\"\"\n",
    "    \n",
    "    return classifier.predict(bowObj.transform([tweet]))[0]\n",
    "\n",
    "# test = \"It's so bad!\"\n",
    "# extract_sentiment(bestClassifier, test, bestBowObj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Question 11 (2 points)\n",
    "\n",
    "Using the *extract_tweet_content*, *detect_airline* and *extract_sentiment*, implement a code that generates a bar chart that contains the number of positive, neutral and negatives tweets for each one of the companies. Briefly describe your bar chart (e.g, which was the company with most negative tweets) and how this chart can help airline companies.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = extract_tweet_content(\"e_corp_dataset.txt\")\n",
    "\n",
    "airline_companies = [\"air france\", \"american airlines\", \"british airways\", \"delta airlines\", \"southwest airlines\", \"united airlines\", \"us airways\", \"virgin america\"]\n",
    "    \n",
    "# Each comany is associated to a triplet counting the number of negative, neutral and positive tweets\n",
    "counts = {}\n",
    "for company in airline_companies:\n",
    "    counts[company] = [0, 0, 0]\n",
    "    \n",
    "for tweet in tweets:\n",
    "    \n",
    "    airlines = detect_airline(tweet)\n",
    "    \n",
    "    # The sentiment is extracted only if airlines are detected\n",
    "    if len(airlines) > 0:\n",
    "        sentiment = extract_sentiment(bestClassifier, tweet, bestBowObj)\n",
    "        for company in airlines:\n",
    "            counts[company][sentiment] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "X = []\n",
    "negative = []\n",
    "neutral = []\n",
    "positive = []\n",
    "\n",
    "for key, value in counts.items():\n",
    "    X.append(key)\n",
    "    negative.append(value[0])\n",
    "    neutral.append(value[1])\n",
    "    positive.append(value[2])\n",
    "\n",
    "# Constructing the bar chart composed of 3 bars (negative, neutral and positive) for each company\n",
    "df = pd.DataFrame(np.c_[negative, neutral, positive], index=X)\n",
    "df.plot.bar(color = [\"red\", \"grey\", \"green\"])\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DESCRIPTION OF BAR CHART**\n",
    "\n",
    "-As we obtained a huge quantity of air france tweets compared to other companies, we decided to check the tweets mentioning the company. We found out that most of them were tweets about the accident of the flight 447 which explains the quantity of tweets and the negativity of the tweets.\n",
    "\n",
    "-For American Airlines, United Airlines, US Airways, the negative tweets dominate, which is supposed to push them to improve their services, perhaps by doing an analysis to detect the complaints that are repeated often, and try to treat them.\n",
    "\n",
    "-For Virgin America and Southwest Airlines , the postitive tweets dominate. The customers are certainly satisfied by these companies.\n",
    "\n",
    "-In fact, we may not have enough tweets for British Airways, Delta Airlines and Southwest Airlines to draw any conclusion about the satisfaction of the customers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 - Term Analysis\n",
    "\n",
    "POS-tagging consists of extracting the part-of-speech (POS) of each token in a sentence. For instance, the table below depicts the part-of-speechs of the sentence *The cat is white!* are.\n",
    "\n",
    "\n",
    "\n",
    "|   The   | cat  |  is  | white     |    !       |\n",
    "|---------|------|------|-----------|------------|\n",
    "| article | noun | verb | adjective | punctation |\n",
    "\n",
    "\n",
    "The part-of-speech can be more complex than what we have learned in the school. Linguistics need to have a more detailed information about systax information of the words in a sentence. For our problem, we do not need this level of information and, thus, we will use a less complex set, called universal POS tags. \n",
    "\n",
    "In POS-tagging, each part-of-speech is represented by a tag. You can find the POS tag list used in this assignement at https://universaldependencies.org/u/pos/ ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DET'), ('cat', 'NOUN'), ('is', 'VERB'), ('white', 'ADJ'), ('!', '.')]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NLTK POS-tagger\n",
    "\n",
    "import nltk\n",
    "\n",
    "\n",
    "#before using pos_tag function, you have to tokenize the sentence.\n",
    "s = ['The', 'cat', 'is',  'white', '!']\n",
    "nltk.pos_tag(s,tagset='universal')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 Question 12 (2 points)\n",
    "\n",
    "**Implement a code** that retrieves the top 10 most frequent terms for each airline company. You will only consider the terms that appear in a positive and negative tweets. Besides that, we consider as term:\n",
    "1. Words that are either an adjective or a noun\n",
    "2. n-grams that are composed by adjectives followed by a noun (e.g., dirty place) or a noun followed by another noun (e.g.,sports club).\n",
    "\n",
    "Moreover, **generate a table** with the top 10 most frequent terms and their normalized frequencies(percentage) for each airline company.\n",
    "\n",
    "**Do not forget to remove the company names from the chart.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the tweets\n",
    "tweets = extract_tweet_content(\"e_corp_dataset.txt\")\n",
    "#tweets = extract_tweet_content(\"test.txt\")\n",
    "\n",
    "airline_companies = [\"air france\", \"american airlines\", \"british airways\", \"delta airlines\", \"southwest airlines\", \"united airlines\", \"us airways\", \"virgin america\"]\n",
    "    \n",
    "# For each company, a dictionary composed of terms-counts is stored \n",
    "terms_by_company = {}\n",
    "for company in airline_companies:\n",
    "    terms_by_company[company] = {}\n",
    "\n",
    "pipeline = PreprocessingPipeline(True, True, False)\n",
    "\n",
    "for tweet in tweets:\n",
    "    \n",
    "    airlines = detect_airline(tweet)\n",
    "    \n",
    "    # If at least one company is detected\n",
    "    if len(airlines) > 0:\n",
    "        \n",
    "        sentiment = extract_sentiment(bestClassifier, tweet, bestBowObj)\n",
    "        \n",
    "        # We are not interested in neutral tweets\n",
    "        if sentiment != 1:\n",
    "            \n",
    "            # All the terms in the tweet\n",
    "            terms = []\n",
    "            \n",
    "            tokens = pipeline.preprocess(tweet)\n",
    "            bigrams = bigram(tokens)\n",
    "            trigrams = trigram(tokens)\n",
    "            \n",
    "            # Unigram : adjective or noun\n",
    "            unigram_tags = nltk.pos_tag(tokens, tagset='universal')\n",
    "            for tag in unigram_tags:\n",
    "                if tag[1] == \"ADJ\" or tag[1] == \"NOUN\":\n",
    "                    terms.append(tag[0])\n",
    "            \n",
    "            # Bigram : adjective-noun or noun-noun\n",
    "            for bi in bigrams:\n",
    "                decomposed = bi.split()\n",
    "                decomposed_tags = nltk.pos_tag(decomposed, tagset='universal')\n",
    "                if (decomposed_tags[0][1] == \"ADJ\" and decomposed_tags[1][1] == \"NOUN\") or (decomposed_tags[0][1] == \"NOUN\" and decomposed_tags[1][1] == \"NOUN\"):\n",
    "                    terms.append(bi)\n",
    "            \n",
    "            # Trigram : adjective-adjective-noun\n",
    "            for tri in trigrams:\n",
    "                decomposed = tri.split()\n",
    "                decomposed_tags = nltk.pos_tag(decomposed, tagset='universal')\n",
    "                if decomposed_tags[0][1] == \"ADJ\" and decomposed_tags[1][1] == \"ADJ\" and decomposed_tags[2][1] == \"NOUN\":\n",
    "                    terms.append(tri)\n",
    "            \n",
    "            # Updating the counts for each company detected in the tweet\n",
    "            for company in airlines:\n",
    "                for term in terms:\n",
    "                    # We don't take the name of the company itself\n",
    "                    if term.lower() not in company:\n",
    "                        if term in terms_by_company[company]:\n",
    "                            terms_by_company[company][term] += 1\n",
    "                        else:\n",
    "                            terms_by_company[company][term] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       word1  frequency1       word2  frequency2  \\\n",
      "air france               sad    0.045177      flight    0.036345   \n",
      "american airlines     flight    0.027778        time    0.020833   \n",
      "british airways      Awesome    0.033333         guy    0.033333   \n",
      "delta airlines      samantha    0.100000       james    0.100000   \n",
      "southwest airlines   Boardin    0.013889      flight    0.013889   \n",
      "united airlines       beauty    0.019108       today    0.019108   \n",
      "us airways            flight    0.035398  experience    0.011799   \n",
      "virgin america        flight    0.041237       board    0.020619   \n",
      "\n",
      "                            word3  frequency3     word4  frequency4  \\\n",
      "air france          France flight    0.027514     plane    0.025815   \n",
      "american airlines            quot    0.013889      bags    0.013889   \n",
      "british airways               bag    0.033333  security    0.033333   \n",
      "delta airlines                 Im    0.100000      cool    0.100000   \n",
      "southwest airlines         Dulles    0.013889     Vegas    0.013889   \n",
      "united airlines         Outsource    0.019108      fare    0.012739   \n",
      "us airways                   time    0.011799     Thank    0.008850   \n",
      "virgin america                 SD    0.010309       Fun    0.010309   \n",
      "\n",
      "                         word5  frequency5         word6  frequency6  \\\n",
      "air france            families    0.021399  France plane    0.015965   \n",
      "american airlines      project    0.013889          last    0.013889   \n",
      "british airways     EVERYTHING    0.033333      rucksack    0.033333   \n",
      "delta airlines           tunes    0.100000          coms    0.100000   \n",
      "southwest airlines      Orange    0.013889        County    0.013889   \n",
      "united airlines         people    0.012739          look    0.012739   \n",
      "us airways                  AA    0.008850          help    0.008850   \n",
      "virgin america           times    0.010309          safe    0.010309   \n",
      "\n",
      "                          word7  frequency7            word8  frequency8  \\\n",
      "air france               people    0.014266       passengers    0.012908   \n",
      "american airlines            PE    0.006944               AA    0.006944   \n",
      "british airways     Airways guy    0.033333  swab EVERYTHING    0.033333   \n",
      "delta airlines              nyc    0.100000   samantha james    0.100000   \n",
      "southwest airlines      excited    0.013889           United    0.013889   \n",
      "united airlines            http    0.012739       experience    0.012739   \n",
      "us airways                today    0.008850         customer    0.008850   \n",
      "virgin america              hat    0.010309              new    0.010309   \n",
      "\n",
      "                      word9  frequency9      word10  frequency10  \n",
      "air france            heart    0.011549       crash     0.009511  \n",
      "american airlines      toss    0.006944      mother     0.006944  \n",
      "british airways       staff    0.033333        free     0.033333  \n",
      "delta airlines      n delta    0.100000  cool tunes     0.100000  \n",
      "southwest airlines     next    0.013889        week     0.013889  \n",
      "united airlines        time    0.012739     Flights     0.012739  \n",
      "us airways            NEVER    0.008850     flights     0.008850  \n",
      "virgin america       avatar    0.010309   Fun times     0.010309  \n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Each company is associated to a list of the top 10 words with the 10 frequecies associated\n",
    "table = {}\n",
    "\n",
    "# Titles of the table rows\n",
    "index_companies = []\n",
    "\n",
    "for company in terms_by_company:\n",
    "    \n",
    "    index_companies.append(company)\n",
    "    \n",
    "    # Calculating the frequencies\n",
    "    counts = Counter(terms_by_company[company])\n",
    "    normalized_counts = {term : count / sum(counts.values()) for term, count in counts.items()} #\n",
    "    top10 = Counter(normalized_counts).most_common(10)\n",
    "    \n",
    "    # List : [word, frequeny, word, frequency, ...]\n",
    "    list_top10 = []\n",
    "    for count in top10:\n",
    "        list_top10.append(count[0])\n",
    "        list_top10.append(count[1])\n",
    "        \n",
    "    table[company] = list_top10\n",
    "\n",
    "# 10 columns for the words and 10 columns for the frequencies\n",
    "columnNames = []\n",
    "for i in range(1, 11):\n",
    "    columnNames.append(\"word\" + str(i))\n",
    "    columnNames.append(\"frequency\" + str(i))\n",
    "\n",
    "# Constructing the table\n",
    "df = pd.DataFrame.from_dict(table, orient='index', columns=columnNames)\n",
    "print(df)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 Question 13 (1 point)\n",
    "\n",
    "The table generated in the Question 12 can lead us to any conclusion about each one of the 9 companies? Can we identify specific events that have occured during the data retrieval?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion regarding the table**\n",
    "\n",
    "\n",
    "This table, by linking it with the previous bar chart can be used by companies to detect the most common complaints among its customers. For example, for American Airlines, the most frequent words are flight and time, which can lead to say that this company needs to improve its travel times and possibly manage delays if they exist. We can also invoke the example of British Airways, which has among the most frequent words \"Security\", so maybe the company lacks security measures in his travels. However, the table remains insufficient, since we also need to know the whole analysis of the tweets in which these words appear the most, in other words, each word must be linked to the sentiment analysis: positive, negative, and neutral feelings. This to be able to identify if the term refers to something to improve, or something good to keep. (We can't really know if \"time\" is used in \"just in time\" or \"not in time\" for instance).\n",
    "\n",
    "As explained previously, we discovered that an accident occurred during the data retrieval : the accident of the Air France Flight 447, which explain the most frequent words for Air France (sad, flight, France flight, families...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Bonus (2 points)\n",
    "\n",
    "Person names, companies names and locations are called named entities. Named-entity recognition (NER) is the task of extracting named entities  classifying them using pre-defined categories. In this bonus section, you will use a Named Entity Recognizer to automatically extract named entities from the tweets. This approach is generic enough to retrieve information about other companies or even product and people names.\n",
    "\n",
    "**For the bonus, you are free to use any Named Entity Recognizer that has python wrapper or is implemented in python. Moreover, you have to use the tweet database of the previous section (https://drive.google.com/file/d/1Cuw6Y12Bj91vF_iH49mqPZZfJkY92iBY/view?usp=sharing)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 - Bonus 2 (1 point)\n",
    "\n",
    "Implement a code that generates the table with the top 10 most mentioned named entities in the database (this table has to contain the frequencies of the name entities). After that, generates a bar chart that despicts the number of positive, negative and neutral tweets for each one of these 10 named entities. Briefly describe the results found in the bar chart.\n",
    "\n",
    "*Ignore the named entities related to the following airline companies : Air France, American, British Airways,  Delta, Southwest, United, Us Airways and Virgin America.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ss\n",
      "0\n",
      "bb\n",
      "10000\n",
      "bb\n",
      "     entity  frequency\n",
      "0    London   0.006649\n",
      "1        LA   0.005319\n",
      "2        US   0.004876\n",
      "3       New   0.004433\n",
      "4     David   0.004433\n",
      "5        UK   0.003989\n",
      "6   Chicago   0.003989\n",
      "7      Nick   0.003989\n",
      "8         &   0.003989\n",
      "9    Boston   0.003546\n",
      "10   Google   0.003546\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from nltk.tag import StanfordNERTagger\n",
    "import os\n",
    "java_path = \"C:/Program Files (x86)/Java/jdk1.8.0_161/bin/java.exe\"\n",
    "os.environ['JAVAHOME'] = java_path\n",
    "\n",
    "#tweets = extract_tweet_content(\"e_corp_dataset.txt\")\n",
    "#tweets = extract_tweet_content(\"test.txt\")\n",
    "\n",
    "airline_companies = [\"air france\", \"american airlines\", \"british airways\", \"delta airlines\", \"southwest airlines\", \"united airlines\", \"us airways\", \"virgin america\"]\n",
    "    \n",
    "# Each entity associated with the number of occurences\n",
    "all_entities = {}\n",
    "\n",
    "pipeline = PreprocessingPipeline(True, True, False)\n",
    "\n",
    "st = StanfordNERTagger('C:/Anaconda/data_mining/stanford-ner-2018-10-16/classifiers/english.all.3class.distsim.crf.ser.gz',\n",
    "                           'C:/Anaconda/data_mining/stanford-ner-2018-10-16/stanford-ner.jar', encoding='utf-8')\n",
    "\n",
    "i=0\n",
    "all_tokens = []\n",
    "\n",
    "for tweet in tweets[:20000]:\n",
    "    \n",
    "    tokens = pipeline.preprocess(tweet)\n",
    "    \n",
    "    for token in tokens:\n",
    "        all_tokens.append(token)\n",
    "        \n",
    "    if i%10000 == 0 or i == len(tweets):\n",
    "        classified = st.tag(all_tokens)\n",
    "        for word in classified:\n",
    "            # Classified as \"O\" means that it is NOT an entity\n",
    "            # We don't take the name of the company itself\n",
    "            if word[1] != \"O\" and word[0].lower() not in airline_companies:\n",
    "                if word[0] not in all_entities:\n",
    "                    all_entities[word[0]] = 1\n",
    "                else:\n",
    "                    all_entities[word[0]] += 1\n",
    "        \n",
    "        all_tokens = []\n",
    "        \n",
    "    i+=1\n",
    "    \n",
    "counts = Counter(all_entities)\n",
    "normalized_counts = {entity : count / sum(counts.values()) for entity, count in counts.items()} #\n",
    "\n",
    "# Retrieving the 11 most common entities and their frequency\n",
    "top11 = Counter(normalized_counts).most_common(11)\n",
    "\n",
    "top11_names = []\n",
    "table11 = []\n",
    "for entity in top11:\n",
    "    table11.append(list(entity))\n",
    "    top11_names.append(entity[0])\n",
    "\n",
    "# Table of 2 columns containing the top 11 entities and their frequency\n",
    "df = pd.DataFrame(table11, columns = ['entity', 'frequency'])\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as detect_airline but takes the words to detect as input\n",
    "def detect_entity(tweet, to_detect):\n",
    "    detected = []\n",
    "\n",
    "    for entity in to_detect:\n",
    "        if entity in tweet:\n",
    "            detected.append(entity)\n",
    "    \n",
    "    return detected\n",
    "\n",
    "# Each entity is associated to a triplet (counts of neg, neut, pos)\n",
    "counts = {}\n",
    "for entity in top11_names:\n",
    "    counts[entity] = [0, 0, 0]\n",
    "    \n",
    "for tweet in tweets:\n",
    "    entities = detect_entity(tweet, top11_names)\n",
    "    if len(entities) > 0:\n",
    "        sentiment = extract_sentiment(bestClassifier, tweet, bestBowObj)\n",
    "        for entity in entities:\n",
    "            counts[entity][sentiment] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "X = []\n",
    "negative = []\n",
    "neutral = []\n",
    "positive = []\n",
    "\n",
    "for key, value in counts.items():\n",
    "    X.append(key)\n",
    "    negative.append(value[0])\n",
    "    neutral.append(value[1])\n",
    "    positive.append(value[2])\n",
    "\n",
    "# Constructing the bar chart for the top 10 entities\n",
    "df = pd.DataFrame(np.c_[negative, neutral, positive], index=X)\n",
    "df.plot.bar(color = [\"red\", \"grey\", \"green\"])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(11 entities were displayed because \"&\" should not really count as an entity even if the NER says so)\n",
    "\n",
    "We notice that the named entities most mentioned in the tweets are those of the US states, and unlike the first bar chart, here the positive feeling is more present on the tweets, more present even than the neutral feeling except for Google which is more neutral, maybe because it is used in tweets just to mention \"type on Google\" or \"search on Google\"..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 - Bonus 3 (1 point)\n",
    "\n",
    "Generate a similar table produced in the Question 12 for the 10 most mentioned named entities in Bonus 2. Can we draw any conclusion about these named entities?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAME CODE AS QUESTION 12 BUT USING THE 10 MOST MENTIONED ENTITIES INSTEAD OF AIRLINES\n",
    "\n",
    "tweets = extract_tweet_content(\"e_corp_dataset.txt\")\n",
    "#tweets = extract_tweet_content(\"test.txt\")\n",
    "\n",
    "terms_by_entity = {}\n",
    "for entity in top11_names:\n",
    "    terms_by_entity[entity] = {}\n",
    "\n",
    "pipeline = PreprocessingPipeline(True, True, True)\n",
    "\n",
    "for tweet in tweets:\n",
    "    entities = detect_entity(tweet, top11_names)\n",
    "    if len(entities) > 0:\n",
    "        sentiment = extract_sentiment(bestClassifier, tweet, bestBowObj)\n",
    "        if sentiment != 1:\n",
    "            terms = []\n",
    "            \n",
    "            tokens = pipeline.preprocess(tweet)\n",
    "            \n",
    "            bigrams = bigram(tokens)\n",
    "            trigrams = trigram(tokens)\n",
    "            \n",
    "            unigram_tags = nltk.pos_tag(tokens, tagset='universal')\n",
    "            for tag in unigram_tags:\n",
    "                if tag[1] == \"ADJ\" or tag[1] == \"NOUN\":\n",
    "                    terms.append(tag[0])\n",
    "            \n",
    "            for bi in bigrams:\n",
    "                decomposed = bi.split()\n",
    "                decomposed_tags = nltk.pos_tag(decomposed, tagset='universal')\n",
    "                if (decomposed_tags[0][1] == \"ADJ\" and decomposed_tags[1][1] == \"NOUN\") or (decomposed_tags[0][1] == \"NOUN\" and decomposed_tags[1][1] == \"NOUN\"):\n",
    "                    terms.append(bi)\n",
    "            \n",
    "            for tri in trigrams:\n",
    "                decomposed = tri.split()\n",
    "                decomposed_tags = nltk.pos_tag(decomposed, tagset='universal')\n",
    "                if decomposed_tags[0][1] == \"ADJ\" and decomposed_tags[1][1] == \"ADJ\" and decomposed_tags[2][1] == \"NOUN\":\n",
    "                    terms.append(tri)\n",
    "                    \n",
    "            for entity in entities:\n",
    "                for term in terms:\n",
    "                    if term not in entity:\n",
    "                        if term in terms_by_entity[entity]:\n",
    "                            terms_by_entity[entity][term] += 1\n",
    "                        else:\n",
    "                            terms_by_entity[entity][term] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           word1  frequency1   word2  frequency2 word3  frequency3     word4  \\\n",
      "London    london    0.075162       i    0.066034   day    0.013335      good   \n",
      "LA             i    0.064183      la    0.022319   day    0.007854      last   \n",
      "US             i    0.064793  flight    0.009540  good    0.006379     thank   \n",
      "New          new    0.068773       i    0.045910  moon    0.016410  new moon   \n",
      "David      david    0.050573    good    0.011002  haha    0.007652      cook   \n",
      "UK            uk    0.066448       i    0.061166  good    0.009593       day   \n",
      "Chicago  chicago    0.083527     day    0.009435  good    0.008817      time   \n",
      "Nick        nick    0.049515    good    0.009069  jona    0.008706       amp   \n",
      "&              i    0.057638    quot    0.045774   amp    0.044578        lt   \n",
      "Boston    boston    0.077728       i    0.067269   day    0.009270      good   \n",
      "Google     googl    0.082165       i    0.054835  http    0.013315      wave   \n",
      "\n",
      "         frequency4   word5  frequency5  ...           word7  frequency7  \\\n",
      "London     0.009057    time    0.007916  ...            week    0.006632   \n",
      "LA         0.007007       u    0.006575  ...             lol    0.005134   \n",
      "US         0.006159     day    0.005909  ...            time    0.005277   \n",
      "New        0.016202    good    0.009219  ...        new york    0.009012   \n",
      "David      0.007154    quot    0.006791  ...  david carradin    0.006520   \n",
      "UK         0.006736   thank    0.005497  ...            time    0.004904   \n",
      "Chicago    0.008043  flight    0.007425  ...           today    0.006342   \n",
      "Nick       0.007164       u    0.006620  ...             day    0.006167   \n",
      "&          0.018217    good    0.007916  ...               u    0.005253   \n",
      "Boston     0.007844   great    0.007131  ...            time    0.005942   \n",
      "Google     0.009285    quot    0.005957  ...            work    0.005256   \n",
      "\n",
      "         word8  frequency8    word9  frequency9 word10  frequency10   word11  \\\n",
      "London   great    0.004635     nice    0.003922    way     0.003922     work   \n",
      "LA       laker    0.005116     time    0.005116    amp     0.004810     quot   \n",
      "US         usa    0.005233        u    0.004983   http     0.004792    today   \n",
      "New       http    0.007191  trailer    0.006845    day     0.006684     news   \n",
      "David      sad    0.006474        u    0.006429    lol     0.006384  concert   \n",
      "UK         lol    0.004527     http    0.004419  great     0.004257      amp   \n",
      "Chicago   home    0.005878     show    0.005568   last     0.004950    great   \n",
      "Nick      quot    0.005985    thank    0.005623   http     0.005351      lol   \n",
      "&         time    0.004711       gt    0.004561  thank     0.004052     http   \n",
      "Boston   thank    0.005467    happi    0.004992  today     0.004754      sad   \n",
      "Google   thank    0.004730     cool    0.004730   look     0.004380   chrome   \n",
      "\n",
      "         frequency11  \n",
      "London      0.003851  \n",
      "LA          0.004648  \n",
      "US          0.003778  \n",
      "New         0.005923  \n",
      "David       0.005976  \n",
      "UK          0.004203  \n",
      "Chicago     0.004950  \n",
      "Nick        0.005169  \n",
      "&           0.004002  \n",
      "Boston      0.004516  \n",
      "Google      0.004205  \n",
      "\n",
      "[11 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "table = {}\n",
    "index_entities = []\n",
    "for entity in terms_by_entity:\n",
    "    index_entities.append(company)\n",
    "    counts = Counter(terms_by_entity[entity])\n",
    "    normalized_counts = {term : count / sum(counts.values()) for term, count in counts.items()} #\n",
    "    top11 = Counter(normalized_counts).most_common(11)\n",
    "    list_top11 = []\n",
    "    for count in top11:\n",
    "        list_top11.append(count[0])\n",
    "        list_top11.append(count[1])\n",
    "    if len(list_top11) > 0:\n",
    "        table[entity] = list_top11\n",
    "\n",
    "columnNames = []\n",
    "for i in range(1, 12):\n",
    "    columnNames.append(\"word\" + str(i))\n",
    "    columnNames.append(\"frequency\" + str(i))\n",
    "    \n",
    "df = pd.DataFrame.from_dict(table, orient='index', columns=columnNames)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(11 words are displayed just because the first one is the entity itself)\n",
    "\n",
    "**Can we draw any conclusion about these named entities?**\n",
    "\n",
    "This table can give an idea of the topics frequently discussed on twitter, and we notice that users talk more about places, and if we link this with the most common words, we can say that they may be interested more to travel, which can be a good opportunity for traveling companies, to do well targeted advertising to users of twitter while taking into consideration the most cited places like London and the United States.\n",
    "\n",
    "In addition, among the unit names invoked, there is David who refers to David Carradine, an American actor, director and screenwriter found hanged in his room on 4 JUNE 2009, ie during the data retrieval."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
